llm_name,method,batch_size,max_epochs,lr0,gradient_accumulation_steps,lr_scheduler_type,optimizer_type,identifier,best_val_loss,avg_rouge_l,avg_bertscore
Llama-3.2-3B-Instruct,finetune,1,10,1.00E-04,16,polynomial_decay,adamw,12011789_10,0.3645778248086572,0.381,0.882
Llama-3.2-3B-Instruct,finetune,1,10,1.00E-04,16,linear,adamw,12011791_11,0.364646490290761,0.376,0.886
Llama-3.2-3B-Instruct,finetune,1,15,8.00E-05,16,linear,adamw,12011862_19,0.3657675748690963,0.375,0.884
Llama-3.2-3B,finetune,1,10,1.00E-04,16,polynomial_decay,adamw,12011739_2,0.41032458320260046,0.372,0.883
Llama-3.2-3B-Instruct,finetune,1,10,5.00E-04,16,linear,adamw,12011736_15,0.368067785166204,0.37,0.88
Llama-3.2-3B-Instruct,finetune,1,10,5.00E-04,8,polynomial_decay,adamw,12011811_12,0.3656808340921998,0.366,0.884
Llama-3.2-3B,finetune,1,10,5.00E-04,16,linear,adamw,12011773_7,0.4067465256899595,0.365,0.882
Llama-3.2-3B-Instruct,finetune,1,10,1.00E-04,8,polynomial_decay,adamw,12011786_8,0.36352282091975213,0.364,0.882
Llama-3.2-3B,finetune,1,15,8.00E-05,16,linear,adamw,12011864_17,0.4113640014082193,0.363,0.88
Llama-3.2-3B-Instruct,finetune,1,15,4.00E-05,16,linear,adamw,12011865_18,0.366777578368783,0.363,0.879
Llama-3.2-3B,finetune,1,10,1.00E-04,16,linear,adamw,12011740_3,0.41053074821829794,0.361,0.88
Llama-3.2-3B-Instruct,finetune,1,10,5.00E-04,8,linear,adamw,12011812_13,0.36575381737202406,0.361,0.88
Llama-3.2-3B-Instruct,finetune,1,10,1.00E-04,8,linear,adamw,12011787_9,0.36355177462100985,0.36,0.883
Llama-3.2-3B,finetune,1,10,1.00E-04,8,polynomial_decay,adamw,12011737_0,0.4098186332732439,0.353,0.88
Llama-3.2-3B,finetune,1,10,1.00E-04,8,linear,adamw,12011738_1,0.410139948874712,0.353,0.879
Llama-3.2-3B-Instruct,finetune,1,10,5.00E-04,16,polynomial_decay,adamw,12011816_14,0.3680508280172944,0.352,0.88
Llama-3.2-3B,finetune,1,15,4.00E-05,16,linear,adamw,12011863_16,0.41525096334517003,0.343,0.876
Llama-3.2-3B,finetune,1,10,5.00E-04,16,polynomial_decay,adamw,12011772_6,0.4068762887269258,0.336,0.875
Llama-3.2-3B,finetune,1,10,5.00E-04,8,polynomial_decay,adamw,12011767_4,0.40141278356313703,0.327,0.874
Llama-3.2-3B,finetune,1,10,5.00E-04,8,linear,adamw,12011768_5,0.40141543485224246,0.326,0.875
Llama-3.2-1B-Instruct,finetune,1,10,5.00E-04,16,linear,adamw,12011669_15,0.5238641425967216,0.312,0.868
Llama-3.2-1B-Instruct,finetune,1,10,5.00E-04,8,linear,adamw,12011685_13,0.5219466764479875,0.308,0.868
Llama-3.2-1B-Instruct,finetune,1,10,1.00E-04,16,linear,adamw,12011681_11,0.5306597925722599,0.304,0.868
Llama-3.2-1B-Instruct,finetune,1,10,1.00E-04,8,linear,adamw,12011679_9,0.5271609731018543,0.303,0.867
Llama-3.2-1B-Instruct,finetune,1,10,1.00E-04,8,polynomial_decay,adamw,12011678_8,0.5269383788108826,0.302,0.868
Llama-3.2-1B-Instruct,finetune,1,15,8.00E-05,16,polynomial_decay,adamw,12011902_7,0.5301664516329765,0.296,0.862
Llama-3.2-1B,finetune,1,10,5.00E-04,16,linear,adamw,12011677_7,0.552954676374793,0.296,0.863
Llama-3.2-1B-Instruct,finetune,1,10,5.00E-04,8,polynomial_decay,adamw,12011684_12,0.5220917664468289,0.295,0.863
Llama-3.2-1B-Instruct,finetune,1,10,5.00E-04,16,polynomial_decay,adamw,12011686_14,0.5238493040204049,0.294,0.867
Llama-3.2-1B-Instruct,finetune,1,10,1.00E-04,16,polynomial_decay,adamw,12011680_10,0.5302863180637359,0.294,0.868
Llama-3.2-1B-Instruct,finetune,1,15,4.00E-05,8,polynomial_decay,adamw,12011910_4,0.5319105885922909,0.292,0.861
Llama-3.2-1B,finetune,1,10,5.00E-04,16,polynomial_decay,adamw,12011676_6,0.5531173437833786,0.291,0.863
Llama-3.2-1B,finetune,1,10,5.00E-04,8,linear,adamw,12011675_5,0.5455696702003479,0.286,0.864
Llama-3.2-1B-Instruct,finetune,1,15,8.00E-05,8,polynomial_decay,adamw,12011913_6,0.5294183231890202,0.28,0.859
Llama-3.2-1B,finetune,1,10,1.00E-04,16,linear,adamw,12011673_3,0.5548839423805475,0.271,0.854
Llama-3.2-1B,finetune,1,10,5.00E-04,8,polynomial_decay,adamw,12011674_4,0.5457187097519636,0.269,0.86
Llama-3.2-1B,finetune,1,10,1.00E-04,8,linear,adamw,12011671_1,0.5480933781713248,0.266,0.851
Llama-3.2-1B,finetune,1,15,8.00E-05,16,polynomial_decay,adamw,12011906_3,0.5519821889698505,0.266,0.852
Llama-3.2-1B,finetune,1,10,1.00E-04,16,polynomial_decay,adamw,12011672_2,0.5546720277518034,0.263,0.855
Llama-3.2-1B-Instruct,finetune,1,15,4.00E-05,16,polynomial_decay,adamw,12011911_5,0.5420842282474041,0.263,0.854
Llama-3.2-1B,finetune,1,10,1.00E-04,8,polynomial_decay,adamw,12011670_0,0.5480803176760674,0.258,0.851
Llama-3.2-1B,finetune,1,15,4.00E-05,8,polynomial_decay,adamw,12011903_0,0.5551590200513601,0.234,0.842
Llama-3.2-1B,finetune,1,15,8.00E-05,8,polynomial_decay,adamw,12011905_2,0.5505595955997705,0.232,0.841
Llama-3.2-1B,finetune,1,15,4.00E-05,16,polynomial_decay,adamw,12011904_1,0.5663095153868198,0.192,0.826
