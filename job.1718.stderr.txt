`low_cpu_mem_usage` was None, now default to True since model is quantized.
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Running tokenizer on train dataset:   0%|          | 0/14 [00:00<?, ? examples/s]Running tokenizer on train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:00<00:00, 183.86 examples/s]
Running tokenizer on train dataset:   0%|          | 0/6 [00:00<?, ? examples/s]Running tokenizer on train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 165.98 examples/s]
Training:   0%|          | 0/14 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Training:   7%|â–‹         | 1/14 [00:01<00:15,  1.16s/it]Training:  14%|â–ˆâ–        | 2/14 [00:02<00:12,  1.08s/it]Training:  21%|â–ˆâ–ˆâ–       | 3/14 [00:03<00:11,  1.06s/it]Training:  29%|â–ˆâ–ˆâ–Š       | 4/14 [00:04<00:10,  1.05s/it]Training:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 5/14 [00:04<00:08,  1.10it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 6/14 [00:05<00:07,  1.05it/s]Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 7/14 [00:06<00:06,  1.03it/s]Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 8/14 [00:08<00:06,  1.00s/it]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 9/14 [00:09<00:05,  1.01s/it]Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 10/14 [00:10<00:04,  1.02s/it]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 11/14 [00:11<00:03,  1.02s/it]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 12/14 [00:12<00:02,  1.02s/it]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 13/14 [00:13<00:01,  1.02s/it]
Validation:   0%|          | 0/6 [00:00<?, ?it/s][A
Validation:  17%|â–ˆâ–‹        | 1/6 [00:00<00:04,  1.06it/s][A
Validation:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:01<00:03,  1.09it/s][A
Validation:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:02<00:02,  1.10it/s][A
Validation:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:03<00:01,  1.10it/s][A
Validation:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:04<00:00,  1.16it/s][A
Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.14it/s][AValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.12it/s]
/home/bulut/miniconda3/envs/hfenv/lib/python3.12/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:23<00:00,  3.81s/it]Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:23<00:00,  1.67s/it]
`low_cpu_mem_usage` was None, now default to True since model is quantized.
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Generating summaries with Llama-3.2-1B:   0%|          | 0/6 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Generating summaries with Llama-3.2-1B:  17%|â–ˆâ–‹        | 1/6 [00:21<01:45, 21.14s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
