{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty gpu cache\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Allocations           |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "import peft\n",
    "import transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_dir = os.getcwd()\n",
    "model_dir = os.path.join(notebook_dir, \"..\", \"..\", \"..\", \"..\", \"local-models/Llama-3.2-1B\")\n",
    "adapter_dir = \"/xdisk/bethard/kbozler/repositories/discharge-summarization/output/adapters/Llama-3.2-1B\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_path):\n",
    "    ''' load model and tokenizer '''\n",
    "\n",
    "    # set quantization configs if using qlora\n",
    "    quantization_config = transformers.BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16\n",
    "            )\n",
    "\n",
    "    # define model and tokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path,\n",
    "                                                    quantization_config=quantization_config)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def get_lora_model(model, device):\n",
    "    ''' add peft adapter to model '''\n",
    "\n",
    "    task_type = peft.TaskType.CAUSAL_LM\n",
    "    \n",
    "    # prepare for k-bit training\n",
    "    model = peft.prepare_model_for_kbit_training(model) \n",
    "    \n",
    "    # get peft configs based on architecture (task_type) and fine-tuning method\n",
    "    config = peft.LoraConfig(   \n",
    "                                task_type=task_type, \n",
    "                                inference_mode=False,\n",
    "                                r=8, \n",
    "                                lora_alpha=32,\n",
    "                                lora_dropout=0.1\n",
    "                            )\n",
    "\n",
    "    # wrap model w peft configs\n",
    "    model = peft.get_peft_model(model, config).to(device)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128257, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=128257, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "base_model, tokenizer = load_model_and_tokenizer(model_dir)\n",
    "model = peft.PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    adapter_dir,\n",
    "    is_trainable=False\n",
    "    )\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"/xdisk/bethard/kbozler/repositories/discharge-summarization/data/processed/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_inference_local(model, tokenizer, test_sample):\n",
    "    \"\"\"\n",
    "    Generates a single summary using the local model\n",
    "    Args:\n",
    "        model (transformers.AutoModelForCausalLM): model to use\n",
    "        test_sample (str): sample to generate a summary for\n",
    "\n",
    "    Returns:\n",
    "        str: generated summary\n",
    "    \"\"\"\n",
    "    # generate summary\n",
    "    device = model.device\n",
    "    inputs = tokenizer(test_sample, return_tensors=\"pt\")\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    print(\"device\", inputs[\"input_ids\"].device)\n",
    "    outputs = model.generate(inputs[\"input_ids\"],\n",
    "                            tokenizer=tokenizer,\n",
    "                            min_new_tokens=250,\n",
    "                            max_new_tokens=500,\n",
    "                            temperature=0.8,\n",
    "                            top_p=0.9,\n",
    "                            top_k=40,\n",
    "                            repetition_penalty=1.2,\n",
    "                            stop_strings=[tokenizer.eos_token]\n",
    "                            ) #check stop strings class \n",
    "    \n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(result)\n",
    "    return result[len(test_sample):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries with Llama-3.2-1B:   0%|          | 0/20 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries with Llama-3.2-1B:   5%|▌         | 1/20 [00:11<03:41, 11.68s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age: 46. gender: M.  \n",
      "Name:  ___             Unit No:   ___\n",
      " \n",
      "Admission Date:  ___              Discharge Date:   ___\n",
      " \n",
      "Date of Birth:  ___             Sex:   M\n",
      " \n",
      "Service: MEDICINE\n",
      " \n",
      "Allergies: \n",
      "No Known Allergies / Adverse Drug Reactions\n",
      " \n",
      "Attending: ___.\n",
      " \n",
      "Chief Complaint:\n",
      "Alcohol withdrawal\n",
      " \n",
      "Major Surgical or Invasive Procedure:\n",
      "None\n",
      "\n",
      " \n",
      "History of Present Illness:\n",
      "From nightfloat admitting resident ___ presents w/ concern \n",
      "for alcohol withdrawal. Reports a h/o prior withdrawal seizure. \n",
      "Drinks 16 drinks of vodka daily, today has had 6, last at ___. Also complaining of insomnia, seeing flashes of \"demonic \n",
      "faces\" and dreams of being levitated by devils. No auditory \n",
      "hallucinations. No SI or HI. No h/o prior psych diagnoses. He \n",
      "has poor sleep hygiene at baseline and has not been sleeping \n",
      "well recently. He noted after beginning to drink today that he \n",
      "was having dry heaves, relieved with Pepto Bismol. He continued \n",
      "to not tolerate PO and his case worker was concerned that he \n",
      "might have a withdrawal seizure at home unwitnessed (his \n",
      "roommate was inpatient at ___ for detox at the time) and \n",
      "advised he come to the ED which he did.\n",
      "\n",
      "Of note, the patient was admitted to ___ for EtOH withdrawal \n",
      "seizure in ___. He was treated with benzos and had no \n",
      "further seizures in the hospital. He has struggled with \n",
      "alcoholism since he was ___ yo. He has had at least ___ for detox.\n",
      "In the ED, initial VS: 99.4 ___ 16 100% RA. Labs notable \n",
      "for AG 18, PLT 78, EtOH 157, ALT 152, AST 176, Tbili 2.2, neg \n",
      "Utox, UA with trace leuks, 40 ketones, 20 WBC, few bacteria, no \n",
      "epis. Psychiatry was consulted and they felt he did not meet \n",
      "admission for psych inpatient admission or dual diagnosis \n",
      "program and recommended observation. His roommate left the \n",
      "inpatient side AMA to be with him in the ED. He was given Valium \n",
      "30mg PO, Ativan 2mg PO, Zofran 4mg, Thiamine 100mg, ___ MVI, 1g \n",
      "Tylenol, 1 Tums, Cipro 500mg PO, 1L NS. VS at transfer: 98.7 \n",
      "129/92 91 21 100% RA.\n",
      "\n",
      "Currently, he has no complaints. Denies SI, HI, vivid images\n",
      " \n",
      "Past Medical History:\n",
      "Alcohol abuse with history of withdrawal seizures in the past\n",
      "Foot/Hand surgery\n",
      " \n",
      "Social History:\n",
      "___\n",
      "Family History:\n",
      "Grandfather with alcoholism, many other family members with \n",
      "alcoholism\n",
      " \n",
      "Physical Exam:\n",
      "Admission exam documented by admitting ___ resident;\n",
      "VS - Temp ___, BP 135/90, HR 86, R 18, O2-sat 100% RA\n",
      "GENERAL - well-appearing man in NAD, comfortable, appropriate\n",
      "HEENT - NC/AT, PERRLA, EOMI, sclerae anicteric, MMM, poor \n",
      "dentition, thrush on tongue\n",
      "NECK - supple, no thyromegaly, no JVD, no carotid bruits\n",
      "LUNGS - CTA bilat, no r/rh/wh, good air movement, resp \n",
      "unlabored, no accessory muscle use\n",
      "HEART - PMI non-displaced, fast but regular rhythm, no MRG, nl \n",
      "S1-S2\n",
      "ABDOMEN - NABS, soft/NT/ND, no masses or HSM, no \n",
      "rebound/guarding\n",
      "EXTREMITIES - WWP, no c/c/e, 2+ peripheral pulses (radials, DPs)\n",
      "SKIN - no rashes or lesions\n",
      "LYMPH - no cervical, axillary, or inguinal LAD\n",
      "NEURO - awake, A&Ox3, trace fine tremor, slightly anxious at \n",
      "times\n",
      ".\n",
      "Discharge:\n",
      "VS - 98.3  119/80  84  18  100/RA\n",
      "Gen - thin male, wearing street clothes, lying in bed in NAD\n",
      "HEENT - MMM\n",
      "Heart - RRR, normal S1/S2\n",
      "Lungs - clear b/l, no rales/rhonchi/wheezes\n",
      "ext - no edema\n",
      "skin - no new lesions\n",
      "\n",
      " \n",
      "Pertinent Results:\n",
      "Adm:\n",
      "___ 02:50AM BLOOD WBC-6.0 RBC-4.43* Hgb-15.0 Hct-43.2 \n",
      "MCV-98# MCH-33.9* MCHC-34.8 RDW-12.7 Plt Ct-78*#\n",
      "___ 02:50AM BLOOD Neuts-59.3 ___ Monos-3.9 Eos-0.4 \n",
      "Baso-1.5\n",
      "___ 02:50AM BLOOD Glucose-96 UreaN-13 Creat-0.7 Na-137 \n",
      "K-3.3 Cl-93* HCO3-26 AnGap-21*\n",
      "___ 02:50AM BLOOD ALT-152* AST-176* AlkPhos-69 TotBili-2.2*\n",
      "___ 02:50AM BLOOD Lipase-33\n",
      "___ 02:50AM BLOOD Albumin-5.6* Calcium-9.6 Phos-2.9 Mg-1.7\n",
      "___ 02:50AM BLOOD ASA-6.6 ___ Acetmnp-NEG \n",
      "Bnzodzp-NEG Barbitr-NEG Tricycl-NEG\n",
      "___ 02:50AM URINE Color-DkAmb Appear-Clear Sp ___\n",
      "___ 02:50AM URINE Blood-MOD Nitrite-NEG Protein-300 \n",
      "Glucose-NEG Ketone-40 Bilirub-NEG Urobiln-2* pH-6.0 Leuks-TR\n",
      "___ 02:50AM URINE RBC-3* WBC-20* Bacteri-FEW Yeast-NONE \n",
      "Epi-0\n",
      "___ 02:50AM URINE bnzodzp-NEG barbitr-NEG opiates-NEG \n",
      "cocaine-NEG amphetm-NEG mthdone-NEG\n",
      "\n",
      "Discharge: None\n",
      "\n",
      "MICRO: ___ UCx: No growth (final)\n",
      "___ HIV: Negative\n",
      "___ HCV: Negative\n",
      "\n",
      "STUDIES: ___ Abdominal ultrasound:\n",
      "IMPRESSION: Increased liver echogenicity, compatible with fatty \n",
      "deposition.  No discrete hepatic lesion. \n",
      "\n",
      " \n",
      "Brief Hospital Course:\n",
      "SUMMARY:  ___ year old man with alcohol abuse admitted for \n",
      "detoxification.\n",
      "\n",
      "# Alcohol withdrawal:  Patient is high risk for seizures given \n",
      "high EtOH intake and history of withdrawal seizures. Was \n",
      "monitored on CIWA scale. Received IV thiamine for 24 hours, \n",
      "followed by daily thiamine in addition to MVI and folic acid.  \n",
      "Social work was consulted, and the patient was provided a \n",
      "detailed list of outpatient treatment centers.  He is to start \n",
      "ad care within one week of discharge, and plans to be seen at \n",
      "___ for counseling within several days of discharge. \n",
      "THe importance of aggressive outpatient compliance and \n",
      "counseling was stressed to the patient. Mr. ___ refused \n",
      "inpatient alcohol abuse treatment. \n",
      "\n",
      "# Leukoplakia: Patient noted to have mild leukoplakia at the \n",
      "back of his tongue. Was treated with 5 day course of nystatin.  \n",
      "HIV test was negative.\n",
      "\n",
      "# Thrombocytopenia:  Likely secondary to heavy alcohol abuse, \n",
      "with improving trend at discharge.\n",
      "\n",
      "# Transaminitis: Evidence for fatty deposition noted on \n",
      "ultrasound, likely steatohepatitis from alcohol use. Trend was \n",
      "improving at time of discharge. HCV was negative, HIV negative, \n",
      "and HBV tested in ___ was notable for HBV core and surface \n",
      "antibody positive.\n",
      ".\n",
      "=========\n",
      "TRANSITIONAL ISSUES:\n",
      "=========\n",
      "-Encourage sobriety\n",
      "-Patient strongly encouraged to obtain treatment at outpatient \n",
      "alcohol treatment center and was provided with an appointment \n",
      "for intake at ___ program ___ by Social Work. \n",
      " \n",
      "Medications on Admission:\n",
      "Preadmission medications listed are correct and complete.  \n",
      "Information was obtained from Patient.\n",
      "1. Bismuth Subsalicylate 15 mL PO QID:PRN dyspepsia \n",
      "\n",
      " \n",
      "Discharge Medications:\n",
      "1. Bismuth Subsalicylate 15 mL PO QID:PRN dyspepsia \n",
      "2. FoLIC Acid 1 mg PO DAILY \n",
      "RX *folic acid 1 mg 1 Tablet(s) by mouth daily Disp #*30 Capsule \n",
      "Refills:*0\n",
      "3. Multivitamins 1 TAB PO DAILY \n",
      "RX *Daily Multi-Vitamin   1 Tablet(s) by mouth daily Disp #*30 \n",
      "Capsule Refills:*0\n",
      "4. Thiamine 100 mg PO DAILY \n",
      "RX *thiamine HCl 100 mg 1 Tablet(s) by mouth daily Disp #*30 \n",
      "Capsule Refills:*0\n",
      "5. Nystatin Oral Suspension 5 mL PO QID Duration: 5 Days \n",
      "RX *nystatin 100,000 unit/mL ___ ml by mouth three times daily \n",
      "Disp #*1 Bottle Refills:*0\n",
      "\n",
      " \n",
      "Discharge Disposition:\n",
      "Home\n",
      " \n",
      "Discharge Diagnosis:\n",
      "Alcohol abuse\n",
      "\n",
      " \n",
      "Discharge Condition:\n",
      "Mental Status: Clear and coherent.\n",
      "Level of Consciousness: Alert and interactive.\n",
      "Activity Status: Ambulatory - Independent.\n",
      "\n",
      " \n",
      "Discharge Instructions:\n",
      "You were admitted for alcohol detoxification. We treated you \n",
      "with medicines to prevent dangerous withdrawal symptoms and side \n",
      "effects.\n",
      "\n",
      "We also had out social worker and addiction specialists work \n",
      "with you to come up with a safe plan for discharge to keep you \n",
      "sober.  As you know, it is extremely important that you stop \n",
      "drinking to prevent the many terrible effects alcohol can have \n",
      "on the body.  As we discussed, this will be difficult, but with \n",
      "the proper social supports it is something that you have the \n",
      "capability to do.  By continuing to drink, you are only \n",
      "increasing the chances of death at a young age.\n",
      ".\n",
      "We also tested you for HIV and Hepatitis C, both of which were \n",
      "negative.  You had some findings in your mouth that may have \n",
      "been thrush (a mild infection), so you should finish 5 days of \n",
      "nystatin to treat this.  We have also started you on some \n",
      "vitamins, which you should continue to take.\n",
      ".\n",
      "Please note the following medication changes:\n",
      "-Please START thiamine\n",
      "-Please START multivitamin\n",
      "-Please START folic acid\n",
      "-Please START nystatin for 2 more days\n",
      "\n",
      " \n",
      "Followup Instructions:\n",
      "___\n",
      " \n",
      "\n",
      "To make sure you stay sober when discharged from here,\n",
      "please see us again soon if possible!  For now please call our office \n",
      "if you feel like relapsing back into drinking.\n",
      "\n",
      "\n",
      "Thank you.\n",
      "\n",
      "\n",
      "\n",
      "______________________\n",
      "\n",
      "\n",
      "\n",
      "_______________\n",
      "\n",
      "The above transcript represents information received from the medical record as reported to the staff nurse who cared for the patient during the period specified. The original paper document remains under lock behind reception desk; however, subsequent revisions made upon request by doctor's orders remain available through nursing documentation. Nursing notes and chart summaries prepared before these modifications would appear in printed format below, although clinical content may differ markedly due to actual practice setting limitations imposed by HIPAA confidentiality requirements.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Patient ID Number: _____\n",
      "\n",
      "\n",
      "Reviser Name:\n",
      "\n",
      "\n",
      "Medical Record #: ___________________________\n",
      "\n",
      "Start Time:\n",
      "\n",
      "\n",
      "\n",
      "End Time:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "END OF TRANSCRIPT\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note: This electronic version of documents generated using IBM software products and solutions described herein, including those created during transmission, processing, storage and retrieval via Internet communication systems such as e-mail or Web-based services applications. Such communications are subject to applicable legal restrictions. Except where expressly indicated otherwise, all text contained therein does not constitute legal advice nor constitutes a representation or warranty regarding any person or entity, whether civil, criminal, administrative or governmental. If necessary, law enforcement authorities must determine the validity of specific claims presented. Information transmitted electronically between parties shall contain sufficient material for determination of facts relevant thereto. Neither party assumes responsibility for its contents unless specifically stated explicitly elsewhere.\n",
      "\n",
      "test sample 2 of 20\n",
      "device cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries with Llama-3.2-1B:  10%|█         | 2/20 [00:29<04:33, 15.19s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age: 79. gender: F.  \n",
      "Name:  ___                   Unit No:   ___\n",
      " \n",
      "Admission Date:  ___              Discharge Date:   ___\n",
      " \n",
      "Date of Birth:  ___             Sex:   F\n",
      " \n",
      "Service: PLASTIC\n",
      " \n",
      "Allergies: \n",
      "Penicillins / trees / rye grass\n",
      " \n",
      "Attending: ___\n",
      " \n",
      "___ Complaint:\n",
      "Breast cancer\n",
      " \n",
      "Major Surgical or Invasive Procedure:\n",
      "Left simple mastectomy and sentinel lymph node biopsy, with \n",
      "immediate tissue expander insertion\n",
      "\n",
      " \n",
      "History of Present Illness:\n",
      "___ year old female with history of previous bilateral breast \n",
      "cancer, BCT on left, MRM on right. Now with recurrent left \n",
      "breast cancer.\n",
      "\n",
      " \n",
      "Past Medical History:\n",
      "HTN\n",
      "macular degeneration\n",
      "breast cancer\n",
      "Compression fracture T7 ___ s/p fall\n",
      "Thyroid disease\n",
      "reflux \n",
      " \n",
      "Social History:\n",
      "Non-smoker, occasional ETOH, denies recreational drug use.\n",
      " \n",
      "Physical Exam:\n",
      "Pre-procedure physical exam as documented in anesthesia record \n",
      "___:\n",
      "pulse: 65/min\n",
      "BP: 135/71\n",
      "O2sat: 98% RA\n",
      ".\n",
      "General: NAD\n",
      "Mental/psych: AxOx3\n",
      "Airway: as documented in detail in anesthesia record\n",
      "Dental: Other (chipped R, upper incisor, crowns left and right \n",
      "lower, perm, bridge left upper)\n",
      "Head/neck range of motion: limited\n",
      "Lungs: clear to auscultation\n",
      "Other: anicteric, no LAD, thyroid not palpable; left breast skin \n",
      "no excoriation; breast exam per Dr. ___\n",
      " \n",
      "___ Hospital Course:\n",
      "The patient was admitted to the plastic surgery service on \n",
      "___ and had a left mastectomy with tissue expander \n",
      "placement and sentinel node lymph biopsy.  The patient tolerated \n",
      "the procedure well. \n",
      ".\n",
      "Neuro: Post-operatively, the patient received dilaudid PCA with \n",
      "good effect and adequate pain control. When tolerating oral \n",
      "intake, the patient was transitioned to oral pain medications. \n",
      ".\n",
      "CV: The patient was stable from a cardiovascular standpoint; \n",
      "vital signs were routinely monitored.\n",
      ".\n",
      "Pulmonary: The patient was stable from a pulmonary standpoint; \n",
      "vital signs were routinely monitored.\n",
      ".\n",
      "GI/GU: Post-operatively, the patient was given IV fluids until \n",
      "tolerating oral intake. Her diet was advanced when appropriate, \n",
      "which was tolerated well. She was also started on a bowel \n",
      "regimen to encourage bowel movement. Intake and output were \n",
      "closely monitored. \n",
      ".\n",
      "ID: Post-operatively, the patient was started on IV cefazolin, \n",
      "then switched to PO cefadroxil for discharge home. The patient's \n",
      "temperature was closely watched for signs of infection. \n",
      ".\n",
      "Prophylaxis: The patient was encouraged to get up and ambulate \n",
      "as early as possible. \n",
      ".\n",
      "At the time of discharge on POD#1, the patient was doing well, \n",
      "afebrile with stable vital signs, tolerating a regular diet, \n",
      "ambulating, voiding without assistance, and pain was well \n",
      "controlled.  Her left mastectomy incision was clean and dry, \n",
      "flat.  No evidence of hematoma. Her JP drains x 2 with serosang \n",
      "fluid.\n",
      " \n",
      "Medications on Admission:\n",
      "Lisinopril\n",
      " \n",
      "Discharge Medications:\n",
      "1. lisinopril 5 mg Tablet Sig: 0.5 Tablet PO DAILY (Daily).  \n",
      "2. oxycodone 5 mg Tablet Sig: ___ Tablets PO Q4H (every 4 hours) \n",
      "as needed for pain: Patient given Rx by Dr. ___ \n",
      "previously.  \n",
      "3. Tylenol ___ mg Tablet Sig: ___ Tablets PO every ___ hours as \n",
      "needed for fever or pain: Max 12/day. Do not exceed 4gms/4000mgs \n",
      "of tylenol per day.  \n",
      "4. docusate sodium 100 mg Capsule Sig: One (1) Capsule PO BID (2 \n",
      "times a day): Over the counter stool softener.  \n",
      "5. cefadroxil 500 mg Capsule Sig: One (1) Capsule PO twice a day \n",
      "for 10 days: Continue while drain remains in place. Refill as \n",
      "needed.\n",
      "Disp:*20 Capsule(s)* Refills:*1*\n",
      "\n",
      " \n",
      "Discharge Disposition:\n",
      "Home With Service\n",
      " \n",
      "Facility:\n",
      "___\n",
      " \n",
      "Discharge Diagnosis:\n",
      "Breast cancer\n",
      "\n",
      " \n",
      "Discharge Condition:\n",
      "Mental Status: Clear and coherent.\n",
      "Level of Consciousness: Alert and interactive.\n",
      "Activity Status: Ambulatory - Independent.\n",
      "\n",
      " \n",
      "Discharge Instructions:\n",
      "Personal Care:  \n",
      " 1. Any left mastectomy/chest dressing should be left in place x \n",
      "48 hours after surgery.  \n",
      " 2. Clean around the drain site(s), where the tubing exits the \n",
      "skin, with soap and water.  \n",
      " 3. Strip drain tubing, empty bulb(s), and record output(s) ___ \n",
      "times per day.  \n",
      " 4. A written record of the daily output from each drain should \n",
      "be brought to every follow-up appointment. your drains will be \n",
      "removed as soon as possible when the daily output tapers off to \n",
      "an acceptable amount.  \n",
      " 5. You may shower daily. No baths until instructed to do so by \n",
      "Dr. ___.  When you shower you should secure your drain to a \n",
      "laniard or an old pair of stockings tied around your neck so it \n",
      "does not hang down.\n",
      " 6. You should wear your surgibra at all times, except for \n",
      "showering.\n",
      ".  \n",
      " Activity:  \n",
      " 1. You may resume your regular diet.  \n",
      " 2. DO NOT lift anything heavier than 5 pounds or engage in \n",
      "strenuous activity until instructed by Dr. ___.  \n",
      ".  \n",
      " Medications:  \n",
      " 1. Resume your regular medications unless instructed otherwise \n",
      "and take any new meds as ordered.  \n",
      " 2. You may take your prescribed pain medication for moderate to \n",
      "severe pain. You may switch to Tylenol or Extra Strength Tylenol \n",
      "for mild pain as directed on the packaging. Please note that \n",
      "Percocet and Vicodin have Tylenol as an active ingredient so do \n",
      "not take these meds with additional Tylenol.  \n",
      " 4. Take prescription pain medications for pain not relieved by \n",
      "tylenol.  \n",
      " 5. Take your antibiotic as prescribed.  \n",
      " 6. Take Colace, 100 mg by mouth 2 times per day, while taking \n",
      "the prescription pain medication. You may use a different \n",
      "over-the-counter stool softener if you wish.  \n",
      " 7.  Do not drive or operate heavy machinery while taking any \n",
      "narcotic pain medication. You may have constipation when taking \n",
      "narcotic pain medications (oxycodone, percocet, vicodin, \n",
      "hydrocodone, dilaudid, etc.); you should continue drinking \n",
      "fluids, you may take stool softeners, and should eat foods that \n",
      "are high in fiber.  \n",
      ".  \n",
      " Call the office IMMEDIATELY if you have any of the following:  \n",
      " 1. Signs of infection: fever with chills, increased redness, \n",
      "welling, warmth or tenderness at the surgical site, or unusual \n",
      "drainage from the incision(s).  \n",
      " 2. A large amount of bleeding from the incision(s) or drain(s). \n",
      " \n",
      " 3. Fever greater than 101.5 oF  \n",
      " 4. Severe pain NOT relieved by your medication.  \n",
      ".  \n",
      " Return to the ER if:  \n",
      " * If you are vomiting and cannot keep in fluids or your \n",
      "medications.  \n",
      " * If you have shaking chills, fever greater than 101.5 (F) \n",
      "degrees or 38 (C) degrees, increased redness, swelling or \n",
      "discharge from incision, chest pain, shortness of breath, or \n",
      "anything else that is troubling you.  \n",
      " * Any serious change in your symptoms, or any new symptoms that \n",
      "concern you.  \n",
      ".  \n",
      " DRAIN DISCHARGE INSTRUCTIONS  \n",
      " You are being discharged with drains in place. Drain care is a \n",
      "clean procedure. Wash your hands thoroughly with soap and warm \n",
      "water before performing drain care. Perform drainage care twice \n",
      "a day. Try to empty the drain at the same time each day. Pull \n",
      "the stopper out of the drainage bottle and empty the drainage \n",
      "fluid into the measuring cup. Record the amount of drainage \n",
      "fluid on the record sheet. Reestablish drain suction.  \n",
      "\n",
      " \n",
      "Followup Instructions:\n",
      "___\n",
      " \n",
      "\n",
      "Reassess After _____:\n",
      "\n",
      "---\n",
      "```\n",
      "To: [email protected]\n",
      "\n",
      "Subject: RE: Cancer Survivorship Resources\n",
      "\n",
      "Sent From Your Computer\n",
      "\n",
      "This message contains information which is confidential and subject to legal privilege under U.S. law. It is intended only for those addressees listed above who consent through electronic mail transmission to receive this email communication. This message may contain privileged material protected from disclosure because it falls within one of several categories defined by federal regulations known as \"Confidential Treatment.\" As such, recipients must notify us immediately upon receipt whether they want their messages kept confidentially. To do this, please click here to reply directly via e-mail.\n",
      "\n",
      "If you choose to forward content contained herein to anyone, including yourself, we urge immediate action regarding confidentiality requirements established by Federal Regulation §35CFR183§201(c)(S).\n",
      "\n",
      "Your computer system has been configured using standards designed specifically to protect against loss of privacy. However, some conditions prevent complete protection, particularly over insecure networks such as wireless links. We therefore caution users about disclosing personal data or other sensitive information over the Internet.\n",
      "\n",
      "You may opt-out of receiving emails containing news items related to the topic covered below by sending a blank email message to address@hidden.\n",
      "\n",
      "Please Note That Messages Sent By Users May Not Be Reviewed For All Content Before Being Posted On Our Website Or Email System.\n",
      "\n",
      "If you decide to send communications electronically to persons outside our organization, regardless of network location, you agree to assume full responsibility for its contents. Further, YOU AGREE TO BEAR ALL COST AND LIABILITY FOR ANY USE OF YOUR COMPUTER SYSTEM OR NETWORK WHICH MAY RESULT FROM THE COMMUNICATION SENT BY US THROUGH THIS WEBSITE.\n",
      "\n",
      "We Make NO Guarantees Concerning Results Achieved Through Use Of Products Mentioned Within These Web Pages And Accept Absolutely No Liability Associated Therewith Therefore Nothing On These Websites Should Be Interprted As Providing Advice Regarding Specific Matters. While Every Effort Is Made To Ensure Accuracy At Time Of Publication None Shall Be Responsible For Loss Caused Due To Incorrect Information Provided Hereunder Such Errors Are Either Intentional Or Accidental But They Cannot Be Excluded Without Cause.\n",
      "\n",
      "Any statements made concerning products featured throughout various sections shall not be construed as medical advice nor constitute medical opinions expressed by physicians. Nor does it purport to diagnose particular diseases based solely on text alone but rather represents mere speculation thereof.\n",
      "\n",
      "While we make great efforts to ensure the integrity of said website and remove offensive materials whenever notified, there exists potential for human error especially when dealing with\n",
      "device cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries with Llama-3.2-1B:  10%|█         | 2/20 [00:35<05:16, 17.59s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest sample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m text \u001b[38;5;241m=\u001b[39m trial[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m generated_summaries\u001b[38;5;241m.\u001b[39mappend(single_inference_local(model, tokenizer, text))\n",
      "Cell \u001b[0;32mIn[57], line 16\u001b[0m, in \u001b[0;36msingle_inference_local\u001b[0;34m(model, tokenizer, test_sample)\u001b[0m\n\u001b[1;32m     14\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {key: value\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m, inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 16\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     17\u001b[0m                         tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     18\u001b[0m                         min_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m250\u001b[39m,\n\u001b[1;32m     19\u001b[0m                         max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,\n\u001b[1;32m     20\u001b[0m                         temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m,\n\u001b[1;32m     21\u001b[0m                         top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,\n\u001b[1;32m     22\u001b[0m                         top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m,\n\u001b[1;32m     23\u001b[0m                         repetition_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.2\u001b[39m,\n\u001b[1;32m     24\u001b[0m                         stop_strings\u001b[38;5;241m=\u001b[39m[tokenizer\u001b[38;5;241m.\u001b[39meos_token]\n\u001b[1;32m     25\u001b[0m                         ) \u001b[38;5;66;03m#check stop strings class \u001b[39;00m\n\u001b[1;32m     27\u001b[0m result \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[0;32m~/.conda/envs/hfenv/lib/python3.12/site-packages/peft/peft_model.py:1704\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1702\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1703\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1704\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1705\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1706\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/hfenv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/hfenv/lib/python3.12/site-packages/transformers/generation/utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2207\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2208\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2209\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2210\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2211\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2212\u001b[0m     )\n\u001b[1;32m   2214\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[1;32m   2216\u001b[0m         input_ids,\n\u001b[1;32m   2217\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[1;32m   2218\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[1;32m   2219\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[1;32m   2220\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[1;32m   2221\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[1;32m   2222\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2223\u001b[0m     )\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2226\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2227\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2228\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2229\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2234\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2235\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/hfenv/lib/python3.12/site-packages/transformers/generation/utils.py:3206\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3203\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3205\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 3206\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   3208\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3209\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3210\u001b[0m     outputs,\n\u001b[1;32m   3211\u001b[0m     model_kwargs,\n\u001b[1;32m   3212\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3213\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/hfenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/hfenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/hfenv/lib/python3.12/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.conda/envs/hfenv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:1190\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1187\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1190\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m   1191\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1192\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1193\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1194\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1195\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1196\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1197\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1198\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1199\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1200\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m   1201\u001b[0m )\n\u001b[1;32m   1203\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/hfenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/hfenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/hfenv/lib/python3.12/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.conda/envs/hfenv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:945\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    933\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    934\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    935\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    942\u001b[0m         position_embeddings,\n\u001b[1;32m    943\u001b[0m     )\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 945\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[1;32m    946\u001b[0m         hidden_states,\n\u001b[1;32m    947\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[1;32m    948\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    949\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    950\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    951\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    952\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    953\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[1;32m    954\u001b[0m     )\n\u001b[1;32m    956\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.conda/envs/hfenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/hfenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/hfenv/lib/python3.12/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.conda/envs/hfenv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:692\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    691\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 692\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[1;32m    693\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    695\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/.conda/envs/hfenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/hfenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/hfenv/lib/python3.12/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.conda/envs/hfenv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:258\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    256\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(down_proj)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 258\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(x)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m~/.conda/envs/hfenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/hfenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/hfenv/lib/python3.12/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.conda/envs/hfenv/lib/python3.12/site-packages/bitsandbytes/nn/modules.py:256\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    253\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[1;32m    255\u001b[0m bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[0;32m--> 256\u001b[0m out \u001b[38;5;241m=\u001b[39m bnb\u001b[38;5;241m.\u001b[39mmatmul_4bit(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mt(), bias\u001b[38;5;241m=\u001b[39mbias, quant_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mquant_state)\n\u001b[1;32m    258\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mto(inp_dtype)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.conda/envs/hfenv/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:572\u001b[0m, in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m MatMul4Bit\u001b[38;5;241m.\u001b[39mapply(A, B, out, bias, quant_state)\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 572\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mgemv_4bit(A, B\u001b[38;5;241m.\u001b[39mt(), out, state\u001b[38;5;241m=\u001b[39mquant_state)\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    574\u001b[0m         out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m bias\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "generated_summaries = []\n",
    "for i, trial in tqdm(test_df.iterrows(), total=test_df.shape[0], desc=f\"Generating summaries with Llama-3.2-1B\"):\n",
    "    if (i+1)/test_df.shape[0] % 0.1 == 0:\n",
    "        print(f\"test sample {i+1} of {test_df.shape[0]}\")\n",
    "    text = trial[\"text\"]\n",
    "    generated_summaries.append(single_inference_local(model, tokenizer, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = [\n",
    "    \"A lonely robot found a broken music box in an abandoned city. As he fixed it, the melody attracted other robots. Together, they created the city's first robot orchestra.\",\n",
    "    \"The last seed on Earth was planted by a child in her grandmother's garden. Against all odds, it grew into a magical tree that produced seeds of every plant that had been lost.\",\n",
    "    \"In a world where dreams were visible as floating bubbles, a young girl discovered she could weave them into blankets. Her creations brought comfort to those suffering from nightmares.\",\n",
    "    \"An old lighthouse keeper discovered that his beacon didn't guide ships, but rather lost stars back to their constellations. Each night, he helped rebuild the night sky.\",\n",
    "    \"Deep in the digital forest, a virus learned to heal corrupted files instead of destroying them. Other programs began calling it the Digital Doctor.\",\n",
    "    \"A time-traveling mailman accidentally delivered letters to the wrong centuries. The resulting mix-ups created unexpected friendships across time.\",\n",
    "    \"The last bookstore on Mars housed a librarian who could bring characters to life by reading aloud. She used this gift to help homesick colonists feel less alone.\"\n",
    "]\n",
    "\n",
    "summaries = [\n",
    "    \"A robot repairs a music box and builds community through music.\",\n",
    "    \"A child's last seed miraculously restores Earth's lost plants.\",\n",
    "    \"A girl turns visible dreams into comforting blankets for nightmare sufferers.\",\n",
    "    \"A lighthouse keeper helps lost stars find their way back to constellations.\",\n",
    "    \"A benevolent virus becomes known for healing corrupted files.\",\n",
    "    \"A mailman's time-travel mistakes lead to cross-century friendships.\",\n",
    "    \"A Martian librarian uses her power to comfort colonists with living stories.\"\n",
    "]\n",
    "\n",
    "alternative_stories = [\n",
    "    \"In a forgotten city, a solitary robot stumbled upon an ancient piano. As it played, the harmonious notes summoned other robots, and together they formed a symphony that echoed through the empty streets.\",\n",
    "    \"A young girl planted a mysterious seed in a barren land. To everyone's amazement, it sprouted into a tree that bore fruits of every extinct plant, reviving the world's lost flora.\",\n",
    "    \"In a realm where dreams floated like clouds, a girl learned to capture them in jars. Her bottled dreams provided solace to those haunted by restless nights.\",\n",
    "    \"An old lighthouse keeper discovered that his beacon didn't guide ships, but rather lost stars back to their constellations. Each night, he helped rebuild the night sky.\",\n",
    "\n",
    "]\n",
    "\n",
    "alternative_summaries = [\n",
    "    \"A robot finds a piano and unites others through music.\",\n",
    "    \"A girl's seed grows into a tree that revives extinct plants.\",\n",
    "    \"A girl captures dreams to comfort those with nightmares.\",\n",
    "    \"A lighthouse keeper helps lost stars find their way back to constellations.\",\n",
    "\n",
    "]\n",
    "pairs_train = pd.DataFrame({\"text\": stories, \"target\": summaries})\n",
    "pairs_val = pd.DataFrame({\"text\": alternative_stories, \"target\": alternative_summaries})\n",
    "pairs_train = Dataset.from_pandas(pairs_train)\n",
    "pairs_val = Dataset.from_pandas(pairs_val)\n",
    "pairs = DatasetDict({\"train\": pairs_train, \"validation\": pairs_val})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_function(example, tokenizer):\n",
    "    inputs = tokenizer(example[\"text\"], add_special_tokens=True) # only gonna add bos\n",
    "    targets = tokenizer(example[\"target\"], add_special_tokens=False) # we will manually add eos\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_input_ids =  inputs[\"input_ids\"][i] \n",
    "        sample_label_input_ids = targets[\"input_ids\"][i] + [tokenizer.eos_token_id]\n",
    "        inputs[\"input_ids\"][i] = sample_input_ids + sample_label_input_ids\n",
    "        targets[\"input_ids\"][i] = [-100] * len(sample_input_ids) + sample_label_input_ids\n",
    "        inputs[\"attention_mask\"][i] = [1] * len(inputs[\"input_ids\"][i])\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    # input_ids, attention_mask, and labels are all the same length for a given sample, but not across samples\n",
    "    # so we need to pad to max length from left side\n",
    "\n",
    "    max_length = max([len(x) for x in inputs[\"input_ids\"]])\n",
    "    # add padding tokens to the left side of the input ids, attention mask, and labels\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        inputs[\"input_ids\"][i] = ([tokenizer.pad_token_id] * \n",
    "                                (max_length - len(inputs[\"input_ids\"][i])) + \n",
    "                                inputs[\"input_ids\"][i])\n",
    "        inputs[\"attention_mask\"][i] = ([0] * (max_length - len(inputs[\"attention_mask\"][i])) +\n",
    "                                    inputs[\"attention_mask\"][i])\n",
    "        inputs[\"labels\"][i] = ([-100] * (max_length - len(inputs[\"labels\"][i])) +\n",
    "                                inputs[\"labels\"][i])\n",
    "        \n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_dataset = pairs.map(\n",
    "        lambda x: tokenize_function(x, tokenizer=tokenizer),\n",
    "        batched=True,\n",
    "        remove_columns=pairs['train'].column_names,\n",
    "        load_from_cache_file=False,\n",
    "        desc=\"Running tokenizer on train dataset\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in pairs_dataset:\n",
    "    print(split)\n",
    "    for i in range(len(pairs_dataset[split][\"input_ids\"])):\n",
    "        print(pairs_dataset[split][\"input_ids\"][i])   \n",
    "        print(len(pairs_dataset[split]  [\"input_ids\"][i]))\n",
    "        print(pairs_dataset[split][\"attention_mask\"][i])  \n",
    "        print(len(pairs_dataset[split][\"attention_mask\"][i]))\n",
    "        print(pairs_dataset[split][\"labels\"][i])\n",
    "        print(len(pairs_dataset[split][\"labels\"][i]))\n",
    "        print(\"-\"*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    pairs_dataset['train'], shuffle=True, collate_fn=default_data_collator, batch_size=2\n",
    ")\n",
    "dev_dataloader = DataLoader(\n",
    "    pairs_dataset['validation'], shuffle=True, collate_fn=default_data_collator, batch_size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dev_dataloader:\n",
    "    print(\"-\"*100)\n",
    "    print(\"train batch\")\n",
    "    for i in range(len(batch[\"input_ids\"])):\n",
    "        print(f\"number {i}\")\n",
    "        print(batch[\"input_ids\"][i])   \n",
    "        print(len(batch[\"input_ids\"][i]))\n",
    "        print(batch[\"attention_mask\"][i])  \n",
    "        print(len(batch[\"attention_mask\"][i]))\n",
    "        print(batch[\"labels\"][i])\n",
    "        print(len(batch[\"labels\"][i]))\n",
    "        print(\"-\"*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
