{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import transformers\n",
    "import peft\n",
    "from transformers import default_data_collator\n",
    "import argparse\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths\n",
    "project_path = os.getcwd()\n",
    "data_path = os.path.join(project_path, '..', '..', \"data/raw/Hospitalization-Summarization.json\")\n",
    "save_data_path = os.path.join(project_path, '..', '..', 'data/processed')\n",
    "model_name = 'Llama-3.2-1B'\n",
    "model_path = os.path.join(project_path, '..', '..', '..', model_name)\n",
    "metrics_save_path = os.path.join(project_path, '..', '..', 'results/metrics')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "# clear device cache\n",
    "torch.cuda.empty_cache()\n",
    "# define args\n",
    "args = argparse.Namespace()\n",
    "args.device = device\n",
    "args.batch_size = 1\n",
    "args.max_training_epochs = 2\n",
    "args.patience = 2\n",
    "args.lr0 = 1e-3\n",
    "args.lr_schedule = 'linear_decay'\n",
    "args.lr_num_warmup_steps = 1\n",
    "args.gradient_accumulation_steps = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_path):\n",
    "    ''' load model and tokenizer '''\n",
    "\n",
    "    # set quantization configs if using qlora\n",
    "    quantization_config = transformers.BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16\n",
    "            )\n",
    "\n",
    "    # define model and tokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path,\n",
    "                                                    quantization_config=quantization_config)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    if tokenizer.pad_token_id is None: # autoregressive models' pad token not set by default\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id \n",
    "    \n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def get_tunable_model(model, device):\n",
    "    ''' prep model for param-efficient fine-tuning '''\n",
    "\n",
    "    task_type = peft.TaskType.CAUSAL_LM\n",
    "    \n",
    "    # prepare for k-bit training\n",
    "    model = peft.prepare_model_for_kbit_training(model) \n",
    "    \n",
    "    # get peft configs based on architecture (task_type) and fine-tuning method\n",
    "    config = peft.LoraConfig(task_type=task_type, inference_mode=False,\n",
    "                                r=8, lora_alpha=32,\n",
    "                                lora_dropout=0.1)\n",
    "\n",
    "    # wrap model w peft configs\n",
    "    model = peft.get_peft_model(model, config).to(device)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    return model\n",
    "\n",
    "def preprocess_data(data_path, save_data_path, max_samples=None, split_ratio=0.5):\n",
    "    \"\"\" load data from json file to pandas dataframe \"\"\"\n",
    "    with open(data_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    data = pd.DataFrame(data)\n",
    "    if max_samples is not None:\n",
    "        data = data[:max_samples]\n",
    "    # remove columns except for instruct and answer\n",
    "    data = data[['instruct', 'answer']]\n",
    "    # remove Input:\\n from beginning of instruct texts\n",
    "    data['instruct'] = data['instruct'].apply(lambda x: x.split('Input:\\n')[1] if 'Input:\\n' in x else x)\n",
    "    # change column names to text and target\n",
    "    data.rename(columns={'instruct': 'text', 'answer': 'target'}, inplace=True)\n",
    "    # inject ||startoftext|| and ||endoftext||\n",
    "    data['target'] = data['target'].apply(lambda x: \"||startoftext||\" + x + \" ||endoftext||\")\n",
    "    \n",
    "    # First split data into train and temp (0.5 each)\n",
    "    train_data, temp_data = train_test_split(data, test_size=0.5, random_state=42)\n",
    "    # Split temp data into val and test (0.5 each, resulting in 0.25 of original data each)\n",
    "    val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "    \n",
    "    # save to csv\n",
    "    train_data.to_csv(os.path.join(save_data_path, 'train.csv'))\n",
    "    val_data.to_csv(os.path.join(save_data_path, 'val.csv'))\n",
    "    test_data.to_csv(os.path.join(save_data_path, 'test.csv'))\n",
    "\n",
    "def tokenize_function(examples, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Preprocess function for autoregressive models using left padding to fit max_length.\n",
    "    final version:\n",
    "        input_ids: padding + text + label + eos (no masking, full text and label)\n",
    "        attention_mask: padding + text + label + eos (all 1s except for padding)\n",
    "        labels: padding + label + eos (-100 masks padding and input tokens)\n",
    "    \"\"\"\n",
    "     # Insert special tokens into inputs and targets\n",
    "    batch_size = len(examples['text'])\n",
    "    inputs = examples['text']\n",
    "    targets = examples['target']\n",
    "\n",
    "    # Tokenize inputs and targets without padding\n",
    "    model_inputs = tokenizer(inputs)\n",
    "    labels = tokenizer(targets, add_special_tokens=False)\n",
    "    # print(\"0th model_inputs\", model_inputs[\"input_ids\"][0])\n",
    "    # print(\"length of 0th model_inputs\", len(model_inputs[\"input_ids\"][0]))\n",
    "    # print(\"0th labels\", labels[\"input_ids\"][0])\n",
    "    # print(\"length of 0th labels\", len(labels[\"input_ids\"][0]))\n",
    "\n",
    "    for i in range(batch_size): # for each example input-target pairs\n",
    "        # Get tokenized input and label IDs\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        sample_label_input_ids = labels[\"input_ids\"][i] + [tokenizer.eos_token_id]\n",
    "\n",
    "        # Append labels to inputs\n",
    "        model_inputs[\"input_ids\"][i] = sample_input_ids + sample_label_input_ids\n",
    "\n",
    "        # Create labels for loss computation (-100 masks input tokens)\n",
    "        labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + sample_label_input_ids\n",
    "\n",
    "        # Update attention mask\n",
    "        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n",
    "\n",
    "    # print(\"0th model_inputs\", model_inputs[\"input_ids\"][0])\n",
    "    # print(\"length of 0th model_inputs\", len(model_inputs[\"input_ids\"][0]))\n",
    "    # print(\"0th labels\", labels[\"input_ids\"][0])\n",
    "    # print(\"length of 0th labels\", len(labels[\"input_ids\"][0]))\n",
    "    # print(\"0th attention mask\", model_inputs[\"attention_mask\"][0])\n",
    "    # print(\"length of 0th attention mask\", len(model_inputs[\"attention_mask\"][0]))\n",
    "    # handle padding from left side, to fit max_length\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        sample_label_input_ids = labels[\"input_ids\"][i]\n",
    "        # pad input ids\n",
    "        model_inputs[\"input_ids\"][i] = ([tokenizer.pad_token_id] * \n",
    "                                       (max_length - len(sample_input_ids)) + \n",
    "                                       sample_input_ids)\n",
    "        # pad attention mask\n",
    "        model_inputs[\"attention_mask\"][i] = ([0] * (max_length - len(sample_input_ids)) +\n",
    "                                            model_inputs[\"attention_mask\"][i])\n",
    "        # pad labels\n",
    "        labels[\"input_ids\"][i] = ([-100] * (max_length - len(sample_label_input_ids)) +\n",
    "                                 sample_label_input_ids)\n",
    "    # print(\"0th model_inputs\", model_inputs[\"input_ids\"][0])\n",
    "    # print(\"length of 0th model_inputs\", len(model_inputs[\"input_ids\"][0]))\n",
    "    # print(\"0th labels\", labels[\"input_ids\"][0])\n",
    "    # print(\"length of 0th labels\", len(labels[\"input_ids\"][0]))\n",
    "    # print(\"0th attention mask\", model_inputs[\"attention_mask\"][0])\n",
    "    # print(\"length of 0th attention mask\", len(model_inputs[\"attention_mask\"][0]))\n",
    "    # Add labels to model inputs\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    # truncate to max_length, but im not sure if this is necessary since we already calculated max_length on combined input-target length + 1 for eos token\n",
    "    model_inputs[\"input_ids\"][i] = model_inputs[\"input_ids\"][i][:max_length]\n",
    "    model_inputs[\"attention_mask\"][i] = model_inputs[\"attention_mask\"][i][:max_length]\n",
    "    model_inputs[\"labels\"][i] = model_inputs[\"labels\"][i][:max_length]\n",
    "\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "def calculate_max_length(dataset, tokenizer):\n",
    "    max_lengths = []\n",
    "    \n",
    "    # Calculate max length for each split in the dataset\n",
    "    tokenized_inputs = [tokenizer(example['text'])['input_ids'] for example in dataset]\n",
    "    tokenized_targets = [tokenizer(example['target'])['input_ids'] for example in dataset]\n",
    "\n",
    "    # Get the maximum length from the lengths of each text-text label pair and add 1 for eos token\n",
    "    combined_max_length = max([len(input_ids) + len(label_ids) \n",
    "                              for input_ids, label_ids in zip(tokenized_inputs, tokenized_targets)]) + 1\n",
    "    print(\"combined_max_length\", combined_max_length)\n",
    "    return combined_max_length\n",
    "\n",
    "def get_hf_dataset(data_path):\n",
    "    train_df = pd.read_csv(os.path.join(data_path, 'train.csv'))\n",
    "    val_df = pd.read_csv(os.path.join(data_path, 'val.csv'))\n",
    "    test_df = pd.read_csv(os.path.join(data_path, 'test.csv'))\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    val_dataset = Dataset.from_pandas(val_df)\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "def get_tokenized_datasets(data_path, tokenizer):\n",
    "    train_dataset, val_dataset, test_dataset = get_hf_dataset(data_path)\n",
    "    train_max_length = calculate_max_length(train_dataset, tokenizer)\n",
    "    val_max_length = calculate_max_length(val_dataset, tokenizer)\n",
    "    test_max_length = calculate_max_length(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataset = train_dataset.map(\n",
    "        lambda x: tokenize_function(x, tokenizer=tokenizer, max_length=train_max_length),\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        remove_columns=train_dataset.column_names,\n",
    "        load_from_cache_file=False,\n",
    "        desc=\"Running tokenizer on train dataset\",\n",
    "    )\n",
    "    val_dataset = val_dataset.map(\n",
    "        lambda x: tokenize_function(x, tokenizer=tokenizer, max_length=val_max_length),\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        remove_columns=val_dataset.column_names,\n",
    "        load_from_cache_file=False,\n",
    "        desc=\"Running tokenizer on val dataset\",\n",
    "    )\n",
    "    test_dataset = test_dataset.map(\n",
    "        lambda x: tokenize_function(x, tokenizer=tokenizer, max_length=test_max_length),\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        remove_columns=test_dataset.column_names,\n",
    "        load_from_cache_file=False,\n",
    "        desc=\"Running tokenizer on test dataset\",\n",
    "    )\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "def define_optimizer(args, model):\n",
    "    ''' given parameters\n",
    "        define optimizer '''\n",
    "    \n",
    "    # extract learning rate params\n",
    "    lr0 = args.lr0 # initial learning rate\n",
    "\n",
    "    # define optimizer, lr_scheduler\n",
    "    optimizer = transformers.AdamW(model.parameters(), lr=lr0,\n",
    "                                   no_deprecation_warning=True)\n",
    "\n",
    "    str_ = f'using linear scheduler with lr0 {lr0},'\n",
    "    lr_scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=args.lr_num_warmup_steps,\n",
    "        num_training_steps=args.num_training_steps,\n",
    "    )\n",
    "        \n",
    "    str_ += f' and {args.lr_num_warmup_steps} warm-up steps!' \n",
    "    print(str_)\n",
    "\n",
    "    return optimizer, lr_scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and tokenizer to device    \n",
    "model, tokenizer = load_model_and_tokenizer(model_path)\n",
    "model = get_tunable_model(model, args.device)\n",
    "preprocess_data(data_path, save_data_path, max_samples=16, split_ratio=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = get_tokenized_datasets(save_data_path, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=2, pin_memory=True\n",
    ")\n",
    "dev_dataloader = DataLoader(\n",
    "    val_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=2, pin_memory=True\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=2, pin_memory=True\n",
    ")\n",
    "args.num_training_steps = len(train_dataloader) * args.max_training_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer, lr_scheduler = define_optimizer(args, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the beginning of your script, create a dictionary to store metrics\n",
    "metrics = {\n",
    "    'train_loss': [],\n",
    "    'train_perplexity': [],\n",
    "    'val_loss': [],\n",
    "    'val_perplexity': [],\n",
    "    'learning_rate': [],\n",
    "    'epoch': []\n",
    "}\n",
    "\n",
    "# logging with dictionary updates\n",
    "# In the training loop:\n",
    "model.train()\n",
    "best_val_loss = float('inf')\n",
    "patience = args.patience\n",
    "n_steps = 0\n",
    "trn_losses = []\n",
    "print('begin training!')\n",
    "\n",
    "for epoch in range(args.max_training_epochs):\n",
    "    print(f'epoch {epoch}/{args.max_training_epochs}')\n",
    "    with tqdm(total=len(train_dataloader)) as pbar:\n",
    "        for idx_b, batch in enumerate(train_dataloader):\n",
    "            n_steps += 1\n",
    "            \n",
    "            # forward pass \n",
    "            batch = {k: v.to(args.device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            \n",
    "            # compute loss, gradient step \n",
    "            loss = outputs.loss / args.gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            if (n_steps % args.gradient_accumulation_steps == 0) or (n_steps == len(train_dataloader)):\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            lr_scheduler.step()\n",
    "            \n",
    "            detached_loss = loss.detach().float()\n",
    "            trn_losses.append(detached_loss)\n",
    "            \n",
    "            # Store step-level metrics\n",
    "            metrics['train_loss'].append(float(detached_loss))\n",
    "            metrics['train_perplexity'].append(float(torch.exp(detached_loss)))\n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Validation loop remains same but store metrics\n",
    "    val_losses = []\n",
    "    with tqdm(total=len(dev_dataloader)) as pbar:\n",
    "        for batch in dev_dataloader:\n",
    "            batch = {k: v.to(args.device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs_val = model(**batch)\n",
    "                val_losses.append(outputs_val.loss.detach().float())\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Calculate epoch metrics\n",
    "    trn_loss_epoch = sum(trn_losses) / len(trn_losses)\n",
    "    val_loss_epoch = sum(val_losses) / len(val_losses)\n",
    "    trn_perplexity_epoch = torch.exp(trn_loss_epoch)\n",
    "    val_perplexity_epoch = torch.exp(val_loss_epoch)\n",
    "    \n",
    "    # Store epoch-level metrics\n",
    "    metrics['epoch'].append(epoch)\n",
    "    metrics['learning_rate'].append(float(lr_scheduler.get_lr()[0]))\n",
    "    metrics['val_loss'].append(float(val_loss_epoch))\n",
    "    metrics['val_perplexity'].append(float(val_perplexity_epoch))\n",
    "\n",
    "    print(f\"epoch: {epoch}/{args.max_training_epochs}, \"\n",
    "          f\"trn_loss_epoch: {trn_loss_epoch}, \"\n",
    "          f\"trn_perplexity_epoch: {trn_perplexity_epoch}, \"\n",
    "          f\"val_loss_epoch: {val_loss_epoch}, \"\n",
    "          f\"val_perplexity_epoch: {val_perplexity_epoch}, \"\n",
    "          f\"lr: {lr_scheduler.get_lr()[0]}\")\n",
    "\n",
    "    # Save metrics to JSON file after each epoch\n",
    "    model_epoch_metrics_path = os.path.join(metrics_save_path, f'{model_name}', f'{epoch}')\n",
    "    if not os.path.exists(model_epoch_metrics_path):\n",
    "        os.makedirs(model_epoch_metrics_path)\n",
    "    with open(os.path.join(model_epoch_metrics_path, 'training_metrics.json'), 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "    # early stopping\n",
    "    if val_loss_epoch > best_val_loss:\n",
    "        if patience == 0:\n",
    "            print(f'stopping early at epoch {epoch}!')\n",
    "            break\n",
    "        else:\n",
    "            patience -= 1\n",
    "    else:\n",
    "        patience = args.patience\n",
    "        best_val_loss = val_loss_epoch\n",
    "    \n",
    "# clear device cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
